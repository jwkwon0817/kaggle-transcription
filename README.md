_**본 [KaggleStudy](https://github.com/jwkwon0817/KaggleStudy/)는 [KernelCurriculum](https://kaggle-kr.tistory.com/32)을 참고합니다.**_

_**커널 커리큘럼 작성자 [이유한](https://www.kaggle.com/youhanlee)님**_

#### **Notion: _[Link](https://www.notion.so/invite/1034319fb33d9ba8a27b1b3faaa8b9102cd545dc)_**

### **해당 커리큘럼은 각 커널당 3번씩 필사하는 것을 목표로 합니다.**
### **(+) 각 프로젝트 별 'Most Votes'를 한번 더 필사합니다.**

#

**[Study Member]**
  * **권지원(**_jwkwon0817_**)**
  * **임유경(**_LimYuGyeong_**)**
  * **권혁중(**_DanielKwon_**)**

#### 큰 목차의 제목이 폴더이름
#### 폴더안에 [항목명_GitHub 아이디_작성한 횟수(총 3번 작성해야함).ipynb] 형식으로 저장
##### ex) 1_1_jwkwon0817_1.ipynb <- Binary classification에서 1st level에서 타이타닉 튜토리얼 1을 첫번째 필사함을 의미함.
---

<a href="" target="_blank">

# **Binary classification : Tabular data**

### **[1st level. Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)**
  * [타이타닉 튜토리얼 1 - Exploratory data analysis, visualization, machine learning](https://kaggle-kr.tistory.com/17?category=868316)
    - Most Votes : _[titanic-data-science-solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions)_
  * [EDA To Prediction(DieTanic)](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)
  * [Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)
  * [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)
 
### **[2nd level. Porto Seguro’s Safe Driver Prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)**
  * [Data Preparation & Exploration](https://www.kaggle.com/bertcarremans/data-preparation-exploration)
  * [Interactive Porto Insights - A Plot.ly Tutorial](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)
  * [XGBoost CV (LB .284)](https://www.kaggle.com/aharless/xgboost-cv-lb-284)
  * [Porto Seguro Exploratory Analysis and Prediction](https://www.kaggle.com/gpreda/porto-seguro-exploratory-analysis-and-prediction)
  
### **[3rd level. Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk)**
  * [Introduction: Home Credit Default Risk Competition](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction)
  * [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering)
  * [Stacking Test-Sklearn, XGBoost, CatBoost, LightGBM](https://www.kaggle.com/eliotbarr/stacking-test-sklearn-xgboost-catboost-lightgbm)
  * [LightGBM 7th place solution](https://www.kaggle.com/jsaguiar/lightgbm-7th-place-solution)
  
# **Multi-class classification : Tabular data**

### **[1st level. Costa Rican Household Poverty Level Prediction](https://www.kaggle.com/c/costa-rican-household-poverty-prediction)**
  * [A Complete Introduction and Walkthrough](https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough)
  * [3250feats->532 feats using shap[LB: 0.436]](https://www.kaggle.com/youhanlee/3250feats-532-feats-using-shap-lb-0-436)
  * [XGBoost](https://www.kaggle.com/skooch/xgboost)
  
# **Binary classification : Image classification**

### **[1st level. Statoil/C-CORE Iceberg Classifier Challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)**
  * [Keras Model for Beginners (0.210 on LB)+EDA+R&D](https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d)
  * [Transfer Learning with VGG-16 CNN+AUG LB 0.1712](https://www.kaggle.com/devm2024/transfer-learning-with-vgg-16-cnn-aug-lb-0-1712)
  * [Submarineering.EVEN BETTER PUBLIC SCORE until now.](https://www.kaggle.com/submarineering/submarineering-even-better-public-score-until-now)
  * [Keras+TF LB 0.18](https://www.kaggle.com/wvadim/keras-tf-lb-0-18)
  
# **Multi-class classification : Image classification**

### **[1st level. TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge)**
  * [Speech representation and data exploration](https://www.kaggle.com/davids1992/speech-representation-and-data-exploration)
  * [Light-Weight CNN LB 0.74](https://www.kaggle.com/alphasis/light-weight-cnn-lb-0-74)
  * [WavCeption V1: a 1-D Inception approach (LB 0.76)](https://www.kaggle.com/ivallesp/wavception-v1-a-1-d-inception-approach-lb-0-76)
  
# **Regression : Tabular data**
  
### **[1st level. New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration)**
  * [Dynamics of New York city - Animation](https://www.kaggle.com/drgilermo/dynamics-of-new-york-city-animation)
  * [EDA + Baseline Model](https://www.kaggle.com/aiswaryaramachandran/eda-baseline-model-0-40-rmse)
  * [Beat the benchmark!](https://www.kaggle.com/danijelk/beat-the-benchmark)
  
### **[2nd level. Zillow Prize: Zillow’s Home Value Prediction (Zestimate)](https://www.kaggle.com/c/zillow-prize-1)**
  * [Simple Exploration Notebook - Zillow Prize](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize)
  * [Simple XGBoost Starter (~0.0655)](https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655)
  * [Zillow EDA On Missing Values & Multicollinearity](https://www.kaggle.com/viveksrinivasan/zillow-eda-on-missing-values-multicollinearity)
  * [XGBoost, LightGBM, and OLS and NN](https://www.kaggle.com/aharless/xgboost-lightgbm-and-ols-and-nn)
  
# **Object segmentation : Deep learning**

### **[1st level. 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018)**
  * [Teaching notebook for total imaging newbies](https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies)
  * [Keras U-Net starter - LB 0.277](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277)
  * [Nuclei Overview to Submission](https://www.kaggle.com/kmader/nuclei-overview-to-submission)
  
# **Natural language processing : classification, regression**

### **[1st level. Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification)**
  * [Spooky NLP and Topic Modelling tutorial](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial)
  * [Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)
  * [Simple Feature Engg Notebook - Spooky Author](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author)
  
### **[2nd level. Mercari Price Suggestion Challenge](https://www.kaggle.com/c/mercari-price-suggestion-challenge)**
  * [Mercari Interactive EDA + Topic Modelling](https://www.kaggle.com/thykhuely/mercari-interactive-eda-topic-modelling)
  * [A simple nn solution with Keras (~0.48611 PL)](https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl)
  * [Ridge (LB 0.41943)](https://www.kaggle.com/rumbok/ridge-lb-0-41944)
  * [LGB and FM [18th Place - 0.40604]](https://www.kaggle.com/peterhurford/lgb-and-fm-18th-place-0-40604)
  
### **[3rd level. Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)**
  * [[For Beginners] Tackling Toxic Using Keras](https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras)
  * [Stop the S@#$ - Toxic Comments EDA](https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda)
  * [Logistic regression with words and char n-grams](https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams)
  * [Classifying multi-label comments (0.9741 lb)](https://www.kaggle.com/rhodiumbeng/classifying-multi-label-comments-0-9741-lb)
  
# **Other dataset : anomaly detection, visualization**

### **[1st level. Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)**
  * [In depth skewed data classif. (93% recall acc now)](https://www.kaggle.com/joparga3/in-depth-skewed-data-classif-93-recall-acc-now)
  * [nomaly Detection - Credit Card Fraud Analysis](https://www.kaggle.com/pavansanagapati/anomaly-detection-credit-card-fraud-analysis)
  * [Semi-Supervised Anomaly Detection Survey](https://www.kaggle.com/matheusfacure/semi-supervised-anomaly-detection-survey)
  
### **[2nd level. Kaggle Machine Learning & Data Science Survey 2017](https://www.kaggle.com/kaggle/kaggle-survey-2017)**
  * [Novice to Grandmaster](https://www.kaggle.com/ash316/novice-to-grandmaster)
  * [What do Kagglers say about Data Science?](https://www.kaggle.com/mhajabri/what-do-kagglers-say-about-data-science)
  * [PLOTLY TUTORIAL - 1](https://www.kaggle.com/hakkisimsek/plotly-tutorial-1)
